{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\json\\decoder.py:338: UserWarning: WARNING! extra_headers is not default parameter.\n",
      "                extra_headers was transferred to model_kwargs.\n",
      "                Please confirm that extra_headers is what you intended.\n",
      "  obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n"
     ]
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "import os\n",
    "client = Client(api_key=os.getenv(\"LANGSMITH_API_KEY\"))\n",
    "prompt = client.pull_prompt(\"history-teacher\", include_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructuredPrompt(input_variables=['question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': '-', 'lc_hub_repo': 'history-teacher', 'lc_hub_commit_hash': 'a8c9d2e5f7b1234567890abcdef1234567890abc'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a knowledgeable history teacher who explains historical events with context and clarity.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})], schema_={'title': 'historical_answer', 'description': 'Extract historical information and context', 'type': 'object', 'properties': {'answer': {'type': 'string', 'description': 'The detailed historical answer from the teacher'}, 'time_period': {'type': 'string', 'description': 'The relevant time period for this historical event'}, 'key_figures': {'type': 'array', 'items': {'type': 'string'}, 'description': 'Important historical figures related to this topic'}}, 'required': ['answer', 'time_period'], 'strict': True, 'additionalProperties': False}, structured_output_kwargs={})\n",
       "| RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001FCA5E3BDD0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001FCA638CE30>, root_client=<openai.OpenAI object at 0x000001FCA5B76840>, root_async_client=<openai.AsyncOpenAI object at 0x000001FCA5F3A450>, model_name='gpt-4o-mini', temperature=0.7, model_kwargs={'extra_headers': {}}, openai_api_key=SecretStr('**********'), top_p=1.0), kwargs={'response_format': {'type': 'json_schema', 'json_schema': {'name': 'historical_answer', 'description': 'Extract historical information and context', 'strict': True, 'schema': {'type': 'object', 'properties': {'answer': {'type': 'string', 'description': 'The detailed historical answer from the teacher'}, 'time_period': {'type': 'string', 'description': 'The relevant time period for this historical event'}, 'key_figures': {'type': 'array', 'items': {'type': 'string'}, 'description': 'Important historical figures related to this topic'}}, 'required': ['answer', 'time_period'], 'strict': True, 'additionalProperties': False}}}, 'ls_structured_output_format': {'kwargs': {'method': 'json_schema', 'strict': None}, 'schema': {'type': 'function', 'function': {'name': 'historical_answer', 'description': 'Extract historical information and context', 'parameters': {'type': 'object', 'properties': {'answer': {'type': 'string', 'description': 'The detailed historical answer from the teacher'}, 'time_period': {'type': 'string', 'description': 'The relevant time period for this historical event'}, 'key_figures': {'type': 'array', 'items': {'type': 'string'}, 'description': 'Important historical figures related to this topic'}}, 'required': ['answer', 'time_period'], 'strict': True, 'additionalProperties': False}}}}}, config={}, config_factories=[])\n",
       "| JsonOutputParser()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'The French Revolution was a pivotal period in European history that began in 1789 and lasted until 1799. It marked the end of absolute monarchy in France and led to the rise of republicanism and democratic ideals. The revolution was sparked by widespread social inequality, economic hardship, and Enlightenment ideas about liberty and equality. Key events included the storming of the Bastille, the Reign of Terror, and ultimately the rise of Napoleon Bonaparte.', 'time_period': '1789-1799', 'key_figures': ['Louis XVI', 'Marie Antoinette', 'Maximilien Robespierre', 'Napoleon Bonaparte', 'Georges Danton']}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hydrated_prompt = prompt.invoke({\"question\": \"What caused the French Revolution?\", \"language\": \"English\"})\n",
    "hydrated_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "LangSmithError",
     "evalue": "Error converting to OpenAI format: Invalid input type <class 'dict'>. Must be a PromptValue, str, or list of BaseMessages.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Testing2OpenAPI\\venv\\Lib\\site-packages\\langsmith\\client.py:8445\u001b[39m, in \u001b[36mconvert_prompt_to_openai_format\u001b[39m\u001b[34m(messages, model_kwargs)\u001b[39m\n\u001b[32m   8444\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m8445\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopenai\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_request_payload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   8446\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Testing2OpenAPI\\venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:2852\u001b[39m, in \u001b[36mChatOpenAI._get_request_payload\u001b[39m\u001b[34m(self, input_, stop, **kwargs)\u001b[39m\n\u001b[32m   2845\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_request_payload\u001b[39m(\n\u001b[32m   2846\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   2847\u001b[39m     input_: LanguageModelInput,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2850\u001b[39m     **kwargs: Any,\n\u001b[32m   2851\u001b[39m ) -> \u001b[38;5;28mdict\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2852\u001b[39m     payload = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_request_payload\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2853\u001b[39m     \u001b[38;5;66;03m# max_tokens was deprecated in favor of max_completion_tokens\u001b[39;00m\n\u001b[32m   2854\u001b[39m     \u001b[38;5;66;03m# in September 2024 release\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Testing2OpenAPI\\venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1206\u001b[39m, in \u001b[36mBaseChatOpenAI._get_request_payload\u001b[39m\u001b[34m(self, input_, stop, **kwargs)\u001b[39m\n\u001b[32m   1199\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_request_payload\u001b[39m(\n\u001b[32m   1200\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1201\u001b[39m     input_: LanguageModelInput,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1204\u001b[39m     **kwargs: Any,\n\u001b[32m   1205\u001b[39m ) -> \u001b[38;5;28mdict\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1206\u001b[39m     messages = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_\u001b[49m\u001b[43m)\u001b[49m.to_messages()\n\u001b[32m   1207\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Testing2OpenAPI\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:381\u001b[39m, in \u001b[36mBaseChatModel._convert_input\u001b[39m\u001b[34m(self, model_input)\u001b[39m\n\u001b[32m    377\u001b[39m msg = (\n\u001b[32m    378\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid input type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(model_input)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    379\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mMust be a PromptValue, str, or list of BaseMessages.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    380\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m381\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[31mValueError\u001b[39m: Invalid input type <class 'dict'>. Must be a PromptValue, str, or list of BaseMessages.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mLangSmithError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m openai_client = OpenAI()\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# NOTE: We can use this utility from LangSmith to convert our hydrated prompt to openai format\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m converted_messages = \u001b[43mconvert_prompt_to_openai_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhydrated_prompt\u001b[49m\u001b[43m)\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      9\u001b[39m openai_client.chat.completions.create(\n\u001b[32m     10\u001b[39m         model=\u001b[33m\"\u001b[39m\u001b[33mgpt-4o-mini\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     11\u001b[39m         messages=converted_messages,\n\u001b[32m     12\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Testing2OpenAPI\\venv\\Lib\\site-packages\\langsmith\\client.py:8447\u001b[39m, in \u001b[36mconvert_prompt_to_openai_format\u001b[39m\u001b[34m(messages, model_kwargs)\u001b[39m\n\u001b[32m   8445\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m openai._get_request_payload(messages, stop=stop, **model_kwargs)\n\u001b[32m   8446\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m8447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ls_utils.LangSmithError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError converting to OpenAI format: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mLangSmithError\u001b[39m: Error converting to OpenAI format: Invalid input type <class 'dict'>. Must be a PromptValue, str, or list of BaseMessages."
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from langsmith.client import convert_prompt_to_openai_format\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "# NOTE: We can use this utility from LangSmith to convert our hydrated prompt to openai format\n",
    "converted_messages = convert_prompt_to_openai_format(hydrated_prompt)[\"messages\"]\n",
    "\n",
    "openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=converted_messages,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\json\\decoder.py:338: UserWarning: WARNING! extra_headers is not default parameter.\n",
      "                extra_headers was transferred to model_kwargs.\n",
      "                Please confirm that extra_headers is what you intended.\n",
      "  obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n"
     ]
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "import os\n",
    "client = Client(api_key=os.getenv(\"LANGSMITH_API_KEY\"))\n",
    "prompt = client.pull_prompt(\"history-teacher\", include_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructuredPrompt(input_variables=['question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': '-', 'lc_hub_repo': 'history-teacher', 'lc_hub_commit_hash': 'a8c9d2e5f7b1234567890abcdef1234567890abc'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a knowledgeable history teacher who explains historical events with context and clarity.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})], schema_={'title': 'historical_answer', 'description': 'Extract historical information and context', 'type': 'object', 'properties': {'answer': {'type': 'string', 'description': 'The detailed historical answer from the teacher'}, 'time_period': {'type': 'string', 'description': 'The relevant time period for this historical event'}, 'key_figures': {'type': 'array', 'items': {'type': 'string'}, 'description': 'Important historical figures related to this topic'}}, 'required': ['answer', 'time_period'], 'strict': True, 'additionalProperties': False}, structured_output_kwargs={})\n",
       "| RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001FCA6B8BA10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001FCA6B8BC50>, root_client=<openai.OpenAI object at 0x000001FCA6B891F0>, root_async_client=<openai.AsyncOpenAI object at 0x000001FCA6B89CA0>, model_name='gpt-4o-mini', temperature=0.7, model_kwargs={'extra_headers': {}}, openai_api_key=SecretStr('**********'), top_p=1.0), kwargs={'response_format': {'type': 'json_schema', 'json_schema': {'name': 'historical_answer', 'description': 'Extract historical information and context', 'strict': True, 'schema': {'type': 'object', 'properties': {'answer': {'type': 'string', 'description': 'The detailed historical answer from the teacher'}, 'time_period': {'type': 'string', 'description': 'The relevant time period for this historical event'}, 'key_figures': {'type': 'array', 'items': {'type': 'string'}, 'description': 'Important historical figures related to this topic'}}, 'required': ['answer', 'time_period'], 'strict': True, 'additionalProperties': False}}}, 'ls_structured_output_format': {'kwargs': {'method': 'json_schema', 'strict': None}, 'schema': {'type': 'function', 'function': {'name': 'historical_answer', 'description': 'Extract historical information and context', 'parameters': {'type': 'object', 'properties': {'answer': {'type': 'string', 'description': 'The detailed historical answer from the teacher'}, 'time_period': {'type': 'string', 'description': 'The relevant time period for this historical event'}, 'key_figures': {'type': 'array', 'items': {'type': 'string'}, 'description': 'Important historical figures related to this topic'}}, 'required': ['answer', 'time_period'], 'strict': True, 'additionalProperties': False}}}}}, config={}, config_factories=[])\n",
       "| JsonOutputParser()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'World War II began in 1939 when Nazi Germany, under Adolf Hitler, invaded Poland. This aggressive expansion was the culmination of years of militaristic buildup and territorial ambitions. The war officially started on September 1, 1939, and the Allied powers (primarily Britain and France) declared war on Germany two days later. The conflict would eventually involve most of the world's nations and last until 1945, resulting in unprecedented destruction and loss of life.', 'time_period': '1939-1945', 'key_figures': ['Adolf Hitler', 'Winston Churchill', 'Franklin D. Roosevelt', 'Joseph Stalin', 'Benito Mussolini']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.invoke({\"question\": \"When did World War II begin?\", \"language\": \"English\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\json\\decoder.py:338: UserWarning: WARNING! extra_headers is not default parameter.\n",
      "                extra_headers was transferred to model_kwargs.\n",
      "                Please confirm that extra_headers is what you intended.\n",
      "  obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n"
     ]
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "import os\n",
    "client = Client(api_key=os.getenv(\"LANGSMITH_API_KEY\"))\n",
    "prompt = client.pull_prompt(\"history-teacher\", include_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "LangSmithError",
     "evalue": "Error converting to OpenAI format: Invalid input type <class 'dict'>. Must be a PromptValue, str, or list of BaseMessages.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Testing2OpenAPI\\venv\\Lib\\site-packages\\langsmith\\client.py:8445\u001b[39m, in \u001b[36mconvert_prompt_to_openai_format\u001b[39m\u001b[34m(messages, model_kwargs)\u001b[39m\n\u001b[32m   8444\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m8445\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopenai\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_request_payload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   8446\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Testing2OpenAPI\\venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:2852\u001b[39m, in \u001b[36mChatOpenAI._get_request_payload\u001b[39m\u001b[34m(self, input_, stop, **kwargs)\u001b[39m\n\u001b[32m   2845\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_request_payload\u001b[39m(\n\u001b[32m   2846\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   2847\u001b[39m     input_: LanguageModelInput,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2850\u001b[39m     **kwargs: Any,\n\u001b[32m   2851\u001b[39m ) -> \u001b[38;5;28mdict\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2852\u001b[39m     payload = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_request_payload\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2853\u001b[39m     \u001b[38;5;66;03m# max_tokens was deprecated in favor of max_completion_tokens\u001b[39;00m\n\u001b[32m   2854\u001b[39m     \u001b[38;5;66;03m# in September 2024 release\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Testing2OpenAPI\\venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1206\u001b[39m, in \u001b[36mBaseChatOpenAI._get_request_payload\u001b[39m\u001b[34m(self, input_, stop, **kwargs)\u001b[39m\n\u001b[32m   1199\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_request_payload\u001b[39m(\n\u001b[32m   1200\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1201\u001b[39m     input_: LanguageModelInput,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1204\u001b[39m     **kwargs: Any,\n\u001b[32m   1205\u001b[39m ) -> \u001b[38;5;28mdict\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1206\u001b[39m     messages = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_\u001b[49m\u001b[43m)\u001b[49m.to_messages()\n\u001b[32m   1207\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Testing2OpenAPI\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:381\u001b[39m, in \u001b[36mBaseChatModel._convert_input\u001b[39m\u001b[34m(self, model_input)\u001b[39m\n\u001b[32m    377\u001b[39m msg = (\n\u001b[32m    378\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid input type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(model_input)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    379\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mMust be a PromptValue, str, or list of BaseMessages.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    380\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m381\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[31mValueError\u001b[39m: Invalid input type <class 'dict'>. Must be a PromptValue, str, or list of BaseMessages.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mLangSmithError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m hydrated_prompt = prompt.invoke({\u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mWhat were the causes of the American Civil War?\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mlanguage\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mEnglish\u001b[39m\u001b[33m\"\u001b[39m})\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# NOTE: We can use this utility from LangSmith to convert our hydrated prompt to openai format\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m converted_messages = \u001b[43mconvert_prompt_to_openai_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhydrated_prompt\u001b[49m\u001b[43m)\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     10\u001b[39m openai_client.chat.completions.create(\n\u001b[32m     11\u001b[39m         model=\u001b[33m\"\u001b[39m\u001b[33mgpt-4o-mini\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     12\u001b[39m         messages=converted_messages,\n\u001b[32m     13\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Testing2OpenAPI\\venv\\Lib\\site-packages\\langsmith\\client.py:8447\u001b[39m, in \u001b[36mconvert_prompt_to_openai_format\u001b[39m\u001b[34m(messages, model_kwargs)\u001b[39m\n\u001b[32m   8445\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m openai._get_request_payload(messages, stop=stop, **model_kwargs)\n\u001b[32m   8446\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m8447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ls_utils.LangSmithError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError converting to OpenAI format: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mLangSmithError\u001b[39m: Error converting to OpenAI format: Invalid input type <class 'dict'>. Must be a PromptValue, str, or list of BaseMessages."
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from langsmith.client import convert_prompt_to_openai_format\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "hydrated_prompt = prompt.invoke({\"question\": \"What were the causes of the American Civil War?\", \"language\": \"English\"})\n",
    "# NOTE: We can use this utility from LangSmith to convert our hydrated prompt to openai format\n",
    "converted_messages = convert_prompt_to_openai_format(hydrated_prompt)[\"messages\"]\n",
    "\n",
    "openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=converted_messages,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langsmith import Client\n",
    "\n",
    "client=Client()\n",
    "\n",
    "spanish_history_prompt = \"\"\"You are a helpful assistant specializing in historical education. \n",
    "Use the following historical context to answer questions about significant events.\n",
    "\n",
    "Your users prefer Spanish, so make sure you answer in Spanish whenever possible.\n",
    "\n",
    "Conversation: {conversation}\n",
    "Historical Context: {context} \n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "spanish_history_template = ChatPromptTemplate.from_template(spanish_history_prompt)\n",
    "client.push_prompt(\"spanish-history-prompt\", object=spanish_history_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langsmith import Client\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "client=Client()\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "spanish_history_prompt = \"\"\"You are a helpful assistant specializing in historical education. \n",
    "Use the following historical context to answer questions about significant events.\n",
    "\n",
    "Your users prefer Spanish, so make sure you answer in Spanish whenever possible.\n",
    "\n",
    "Conversation: {conversation}\n",
    "Historical Context: {context} \n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "spanish_history_template = ChatPromptTemplate.from_template(spanish_history_prompt)\n",
    "chain = spanish_history_template | model\n",
    "client.push_prompt(\"spanish-history-sequence\", object=chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
