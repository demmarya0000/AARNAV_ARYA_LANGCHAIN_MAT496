{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or you can use a .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"../../.env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the RAG Application that we've been working with throughout this course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders.sitemap import SitemapLoader\n",
    "from langchain_community.vectorstores import SKLearnVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langsmith import traceable\n",
    "from openai import OpenAI\n",
    "from typing import List\n",
    "import nest_asyncio\n",
    "\n",
    "# TODO: Configure this model!\n",
    "MODEL_NAME = \"gpt-4o\"\n",
    "MODEL_PROVIDER = \"openai\"\n",
    "APP_VERSION = 1.0\n",
    "RAG_SYSTEM_PROMPT = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the latest question in the conversation. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\"\"\"\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "def get_vector_db_retriever():\n",
    "    persist_path = os.path.join(tempfile.gettempdir(), \"union.parquet\")\n",
    "    embd = OpenAIEmbeddings()\n",
    "\n",
    "    # If vector store exists, then load it\n",
    "    if os.path.exists(persist_path):\n",
    "        vectorstore = SKLearnVectorStore(\n",
    "            embedding=embd,\n",
    "            persist_path=persist_path,\n",
    "            serializer=\"parquet\"\n",
    "        )\n",
    "        return vectorstore.as_retriever(lambda_mult=0)\n",
    "\n",
    "    # Otherwise, index LangSmith documents and create new vector store\n",
    "    ls_docs_sitemap_loader = SitemapLoader(web_path=\"https://docs.smith.langchain.com/sitemap.xml\", continue_on_failure=True)\n",
    "    ls_docs = ls_docs_sitemap_loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=500, chunk_overlap=0\n",
    "    )\n",
    "    doc_splits = text_splitter.split_documents(ls_docs)\n",
    "\n",
    "    vectorstore = SKLearnVectorStore.from_documents(\n",
    "        documents=doc_splits,\n",
    "        embedding=embd,\n",
    "        persist_path=persist_path,\n",
    "        serializer=\"parquet\"\n",
    "    )\n",
    "    vectorstore.persist()\n",
    "    return vectorstore.as_retriever(lambda_mult=0)\n",
    "\n",
    "nest_asyncio.apply()\n",
    "retriever = get_vector_db_retriever()\n",
    "\n",
    "\"\"\"\n",
    "retrieve_documents\n",
    "- Returns documents fetched from a vectorstore based on the user's question\n",
    "\"\"\"\n",
    "@traceable(run_type=\"chain\")\n",
    "def retrieve_documents(question: str):\n",
    "    return retriever.invoke(question)\n",
    "\n",
    "\"\"\"\n",
    "generate_response\n",
    "- Calls `call_openai` to generate a model response after formatting inputs\n",
    "\"\"\"\n",
    "@traceable(run_type=\"chain\")\n",
    "def generate_response(question: str, documents):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": RAG_SYSTEM_PROMPT\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"\n",
    "        }\n",
    "    ]\n",
    "    return call_openai(messages)\n",
    "\n",
    "\"\"\"\n",
    "call_openai\n",
    "- Returns the chat completion output from OpenAI\n",
    "\"\"\"\n",
    "@traceable(\n",
    "    run_type=\"llm\",\n",
    "    metadata={\n",
    "        \"ls_provider\": MODEL_PROVIDER,\n",
    "        \"ls_model_name\": MODEL_NAME\n",
    "    }\n",
    ")\n",
    "def call_openai(messages: List[dict]) -> str:\n",
    "    return openai_client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=messages,\n",
    "    )\n",
    "\n",
    "\"\"\"\n",
    "langsmith_rag\n",
    "- Calls `retrieve_documents` to fetch documents\n",
    "- Calls `generate_response` to generate a response based on the fetched documents\n",
    "- Returns the model response\n",
    "\"\"\"\n",
    "@traceable(run_type=\"chain\")\n",
    "def langsmith_rag(question: str):\n",
    "    documents = retrieve_documents(question)\n",
    "    response = generate_response(question, documents)\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith and LangChain serve different purposes within the AI and LLM ecosystems. LangSmith is a platform focused on LLM observability and evaluation, designed for scalability and primarily used for managing and optimizing AI applications. In contrast, LangChain is a framework for building applications powered by language models, with functionalities for various methods such as retrieval and prompt engineering.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "answer = langsmith_rag(\"WHAT IS THE DIFFERENCE BETWEEN LANGSMITH AND LANGCHAIN?\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a code snippet that should look similar to what you see from the starter code!\n",
    "\n",
    "There are a few important components here.\n",
    "\n",
    "1. We have defined an Evaluator\n",
    "2. We pipe our dataset examples (dict) to the shape of input that our function `langsmith_rag` takes (str) using a target function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dataset 'RAG Application Golden Dataset' found with 2 examples\n",
      "View the evaluation results for experiment: 'gpt-4o-4200cf06' at:\n",
      "https://smith.langchain.com/o/679b2151-9692-435f-9a40-bc673cc68d61/datasets/d97e38c5-b6d8-4686-95c7-dcd88d379157/compare?selectedSessions=ddbff85a-6090-4864-b796-c40443b7181b\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:05,  2.86s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.question</th>\n",
       "      <th>outputs.output</th>\n",
       "      <th>error</th>\n",
       "      <th>reference.output</th>\n",
       "      <th>feedback.is_concise</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>example_id</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is LangSmith used for?</td>\n",
       "      <td>LangSmith is used for building production-grad...</td>\n",
       "      <td>None</td>\n",
       "      <td>LangSmith is a platform designed for the devel...</td>\n",
       "      <td>0</td>\n",
       "      <td>2.022388</td>\n",
       "      <td>b6cb2dbe-8d8b-4458-bf94-c37bbabe1d97</td>\n",
       "      <td>c326fa82-da8b-477d-81d8-202fda290707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How do I set up tracing to LangSmith if I'm us...</td>\n",
       "      <td>To set up tracing to LangSmith using LangChain...</td>\n",
       "      <td>None</td>\n",
       "      <td>To set up tracing to LangSmith while using Lan...</td>\n",
       "      <td>0</td>\n",
       "      <td>3.207884</td>\n",
       "      <td>9e85c6d6-adb7-4c3f-ae2c-9b5203b9cbc7</td>\n",
       "      <td>f4dfef81-ad14-41ec-b208-37de1540f989</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<ExperimentResults gpt-4o-4200cf06>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import evaluate, Client\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"RAG Application Golden Dataset\"\n",
    "\n",
    "# Step 1: Check if dataset exists, if not create it\n",
    "try:\n",
    "    dataset = client.read_dataset(dataset_name=dataset_name)\n",
    "    print(f\" Dataset '{dataset_name}' found with {dataset.example_count} examples\")\n",
    "except Exception:\n",
    "    print(f\" Dataset '{dataset_name}' not found. Creating it now...\")\n",
    "    \n",
    "    # Create the dataset\n",
    "    dataset = client.create_dataset(\n",
    "        dataset_name=dataset_name,\n",
    "        description=\"Golden dataset for RAG application evaluation\"\n",
    "    )\n",
    "    \n",
    "    # Add example data\n",
    "    example_inputs = [\n",
    "        (\"How do I set up tracing to LangSmith if I'm using LangChain?\", \"To set up tracing to LangSmith while using LangChain, you need to set the environment variable `LANGSMITH_TRACING` to 'true'. Additionally, you must set the `LANGSMITH_API_KEY` environment variable to your API key.\"),\n",
    "        (\"What is LangSmith used for?\", \"LangSmith is a platform designed for the development, monitoring, and testing of LLM applications.\"),\n",
    "        # Add more examples as needed\n",
    "    ]\n",
    "    \n",
    "    inputs = [{\"question\": q} for q, _ in example_inputs]\n",
    "    outputs = [{\"output\": a} for _, a in example_inputs]\n",
    "    \n",
    "    client.create_examples(\n",
    "        inputs=inputs,\n",
    "        outputs=outputs,\n",
    "        dataset_id=dataset.id,\n",
    "    )\n",
    "    print(f\" Dataset created with {len(example_inputs)} examples\")\n",
    "\n",
    "# Step 2: Define evaluators\n",
    "def is_concise_enough(reference_outputs: dict, outputs: dict) -> dict:\n",
    "    score = len(outputs[\"output\"]) < 1.5 * len(reference_outputs[\"output\"])\n",
    "    return {\"key\": \"is_concise\", \"score\": int(score)}\n",
    "\n",
    "# Step 3: Define target function\n",
    "def target_function(inputs: dict):\n",
    "    return langsmith_rag(inputs[\"question\"])\n",
    "\n",
    "# Step 4: Run evaluation\n",
    "evaluate(\n",
    "    target_function,\n",
    "    data=dataset_name,\n",
    "    evaluators=[is_concise_enough],\n",
    "    experiment_prefix=\"gpt-4o\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'gpt-4o-51d34e6f' at:\n",
      "https://smith.langchain.com/o/679b2151-9692-435f-9a40-bc673cc68d61/datasets/d97e38c5-b6d8-4686-95c7-dcd88d379157/compare?selectedSessions=7fbc4bf1-717f-413f-8e9b-73a46a2d1549\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:07,  3.52s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.question</th>\n",
       "      <th>outputs.output</th>\n",
       "      <th>error</th>\n",
       "      <th>reference.output</th>\n",
       "      <th>feedback.is_concise</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>example_id</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How do I set up tracing to LangSmith if I'm us...</td>\n",
       "      <td>To set up tracing with LangSmith using LangCha...</td>\n",
       "      <td>None</td>\n",
       "      <td>To set up tracing to LangSmith while using Lan...</td>\n",
       "      <td>0</td>\n",
       "      <td>3.533369</td>\n",
       "      <td>9e85c6d6-adb7-4c3f-ae2c-9b5203b9cbc7</td>\n",
       "      <td>4e035484-da9c-436c-987b-32d75a5eb715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is LangSmith used for?</td>\n",
       "      <td>LangSmith is used for building production-grad...</td>\n",
       "      <td>None</td>\n",
       "      <td>LangSmith is a platform designed for the devel...</td>\n",
       "      <td>0</td>\n",
       "      <td>2.836299</td>\n",
       "      <td>b6cb2dbe-8d8b-4458-bf94-c37bbabe1d97</td>\n",
       "      <td>f3945f05-e855-4838-9a72-604e0af1f148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<ExperimentResults gpt-4o-51d34e6f>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import evaluate, Client\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"RAG Application Golden Dataset\"\n",
    "\n",
    "def is_concise_enough(reference_outputs: dict, outputs: dict) -> dict:\n",
    "    score = len(outputs[\"output\"]) < 1.5 * len(reference_outputs[\"output\"])\n",
    "    return {\"key\": \"is_concise\", \"score\": int(score)}\n",
    "\n",
    "def target_function(inputs: dict):\n",
    "    return langsmith_rag(inputs[\"question\"])\n",
    "\n",
    "evaluate(\n",
    "    target_function,\n",
    "    data=dataset_name,\n",
    "    evaluators=[is_concise_enough],\n",
    "    experiment_prefix=\"gpt-4o\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying your Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's change our model to gpt-35-turbo and see how it performs!\n",
    "\n",
    "Make this change, and then run this code snippet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'gpt-3.5-turbo-d8642752' at:\n",
      "https://smith.langchain.com/o/679b2151-9692-435f-9a40-bc673cc68d61/datasets/d97e38c5-b6d8-4686-95c7-dcd88d379157/compare?selectedSessions=87db9346-4dc6-4bc8-9fe2-5e121bda34a5\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:05,  2.61s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.question</th>\n",
       "      <th>outputs.output</th>\n",
       "      <th>error</th>\n",
       "      <th>reference.output</th>\n",
       "      <th>feedback.is_concise</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>example_id</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How do I set up tracing to LangSmith if I'm us...</td>\n",
       "      <td>To set up tracing with LangSmith while using L...</td>\n",
       "      <td>None</td>\n",
       "      <td>To set up tracing to LangSmith while using Lan...</td>\n",
       "      <td>0</td>\n",
       "      <td>2.325508</td>\n",
       "      <td>9e85c6d6-adb7-4c3f-ae2c-9b5203b9cbc7</td>\n",
       "      <td>4e5dc2a2-e031-46ae-91f7-b0d6cdea961a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is LangSmith used for?</td>\n",
       "      <td>LangSmith is a platform used for building prod...</td>\n",
       "      <td>None</td>\n",
       "      <td>LangSmith is a platform designed for the devel...</td>\n",
       "      <td>0</td>\n",
       "      <td>2.406400</td>\n",
       "      <td>b6cb2dbe-8d8b-4458-bf94-c37bbabe1d97</td>\n",
       "      <td>7291544c-b801-463d-a71b-d178cb593575</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<ExperimentResults gpt-3.5-turbo-d8642752>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import evaluate, Client\n",
    "from langsmith.schemas import Example, Run\n",
    "\n",
    "def target_function(inputs: dict):\n",
    "    return langsmith_rag(inputs[\"question\"])\n",
    "\n",
    "evaluate(\n",
    "    target_function,\n",
    "    data=dataset_name,\n",
    "    evaluators=[is_concise_enough],\n",
    "    experiment_prefix=\"gpt-3.5-turbo\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running over Different pieces of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataset Version\n",
    "\n",
    "You can execute an experiment on a specific version of a dataset in the sdk by using the `as_of` parameter in `list_examples`\n",
    "\n",
    "Let's try running on just our initial dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Found existing dataset: RAG Application Golden Dataset\n",
      " Current examples: 2\n",
      " Dataset already contains examples\n",
      " Total examples in dataset: 2\n",
      "\n",
      " Starting evaluation...\n",
      "View the evaluation results for experiment: 'initial dataset version-f7ba2be9' at:\n",
      "https://smith.langchain.com/o/679b2151-9692-435f-9a40-bc673cc68d61/datasets/d97e38c5-b6d8-4686-95c7-dcd88d379157/compare?selectedSessions=30d061a4-e99c-4654-a109-7259da12d156\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [01:56, 58.16s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Evaluation complete!\n",
      " Results: <ExperimentResults initial dataset version-f7ba2be9>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langsmith import evaluate, Client\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"RAG Application Golden Dataset\"\n",
    "\n",
    "# Step 1: Create or get dataset\n",
    "try:\n",
    "    dataset = client.read_dataset(dataset_name=dataset_name)\n",
    "    print(f\" Found existing dataset: {dataset_name}\")\n",
    "except:\n",
    "    dataset = client.create_dataset(\n",
    "        dataset_name=dataset_name,\n",
    "        description=\"Golden dataset for RAG application evaluation\"\n",
    "    )\n",
    "    print(f\" Created new dataset: {dataset_name}\")\n",
    "\n",
    "# Step 2: Add examples if dataset is empty\n",
    "existing_examples = list(client.list_examples(dataset_name=dataset_name))\n",
    "print(f\" Current examples: {len(existing_examples)}\")\n",
    "\n",
    "if len(existing_examples) == 0:\n",
    "    print(\"➕ Adding examples to dataset...\")\n",
    "    \n",
    "    ### examples are added to the file \n",
    "    examples_data = [\n",
    "        {\n",
    "            \"inputs\": {\"question\": \"What is machine learning?\"},\n",
    "            \"outputs\": {\"output\": \"Machine learning is a subset of AI that enables computers to learn from data without explicit programming.\"}\n",
    "        },\n",
    "        {\n",
    "            \"inputs\": {\"question\": \"What is deep learning?\"},\n",
    "            \"outputs\": {\"output\": \"Deep learning uses neural networks with multiple layers to process complex patterns in data.\"}\n",
    "        },\n",
    "        {\n",
    "            \"inputs\": {\"question\": \"What is natural language processing?\"},\n",
    "            \"outputs\": {\"output\": \"NLP is a field that helps computers understand and process human language.\"}\n",
    "        },\n",
    "        {\n",
    "            \"inputs\": {\"question\": \"What is reinforcement learning?\"},\n",
    "            \"outputs\": {\"output\": \"Reinforcement learning trains models through rewards and penalties based on their actions.\"}\n",
    "        },\n",
    "        {\n",
    "            \"inputs\": {\"question\": \"What is a neural network?\"},\n",
    "            \"outputs\": {\"output\": \"A neural network is a computational model inspired by biological brains, consisting of interconnected nodes.\"}\n",
    "        },\n",
    "        {\n",
    "            \"inputs\": {\"question\": \"What is supervised learning?\"},\n",
    "            \"outputs\": {\"output\": \"Supervised learning trains models on labeled data to predict outcomes for new inputs.\"}\n",
    "        },\n",
    "        {\n",
    "            \"inputs\": {\"question\": \"What is unsupervised learning?\"},\n",
    "            \"outputs\": {\"output\": \"Unsupervised learning finds patterns in data without pre-existing labels.\"}\n",
    "        },\n",
    "        {\n",
    "            \"inputs\": {\"question\": \"What is transfer learning?\"},\n",
    "            \"outputs\": {\"output\": \"Transfer learning reuses a pre-trained model as a starting point for a new task.\"}\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Add each example to the dataset\n",
    "    for example in examples_data:\n",
    "        client.create_example(\n",
    "            inputs=example[\"inputs\"],\n",
    "            outputs=example[\"outputs\"],\n",
    "            dataset_id=dataset.id\n",
    "        )\n",
    "    \n",
    "    print(f\" Added {len(examples_data)} examples to dataset\")\n",
    "else:\n",
    "    print(\" Dataset already contains examples\")\n",
    "\n",
    "# Step 3: Verify examples were added\n",
    "examples = list(client.list_examples(dataset_name=dataset_name))\n",
    "print(f\" Total examples in dataset: {len(examples)}\")\n",
    "\n",
    "# Step 4: Define evaluator\n",
    "def is_concise_enough(run, example) -> dict:\n",
    "    \"\"\"Check if output is concise compared to reference\"\"\"\n",
    "    outputs = run.outputs if hasattr(run, 'outputs') else {}\n",
    "    reference_outputs = example.outputs if hasattr(example, 'outputs') else {}\n",
    "    \n",
    "    if not outputs or \"output\" not in outputs:\n",
    "        return {\"key\": \"is_concise\", \"score\": 0}\n",
    "    if not reference_outputs or \"output\" not in reference_outputs:\n",
    "        return {\"key\": \"is_concise\", \"score\": 1}\n",
    "    \n",
    "    score = len(outputs[\"output\"]) < 1.5 * len(reference_outputs[\"output\"])\n",
    "    return {\"key\": \"is_concise\", \"score\": int(score)}\n",
    "\n",
    "# Step 5: Define target function\n",
    "def target_function(inputs: dict):\n",
    "    \"\"\"RAG function that processes questions\"\"\"\n",
    "    question = inputs[\"question\"]\n",
    "    \n",
    "    # TODO: Replace with your actual langsmith_rag implementation\n",
    "    # For now, using a placeholder\n",
    "    try:\n",
    "        result = langsmith_rag(question)\n",
    "        if isinstance(result, str):\n",
    "            return {\"output\": result}\n",
    "        return result\n",
    "    except NameError:\n",
    "        # If langsmith_rag is not defined, use placeholder\n",
    "        return {\"output\": f\"This is a response about {question}\"}\n",
    "\n",
    "# Step 6: Run evaluation\n",
    "if len(examples) == 0:\n",
    "    print(\" Dataset is empty! Cannot run evaluation.\")\n",
    "else:\n",
    "    print(\"\\n Starting evaluation...\")\n",
    "    try:\n",
    "        results = evaluate(\n",
    "            target_function,\n",
    "            data=dataset_name,\n",
    "            evaluators=[is_concise_enough],\n",
    "            experiment_prefix=\"initial dataset version\"\n",
    "        )\n",
    "        print(\" Evaluation complete!\")\n",
    "        print(f\" Results: {results}\")\n",
    "    except Exception as e:\n",
    "        print(f\" Evaluation error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'initial dataset version-b6c17146' at:\n",
      "https://smith.langchain.com/o/679b2151-9692-435f-9a40-bc673cc68d61/datasets/d97e38c5-b6d8-4686-95c7-dcd88d379157/compare?selectedSessions=93690be0-63b3-477c-8add-ae4786d4e012\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:07,  3.66s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.question</th>\n",
       "      <th>outputs.output</th>\n",
       "      <th>error</th>\n",
       "      <th>reference.output</th>\n",
       "      <th>feedback.is_concise</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>example_id</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How do I set up tracing to LangSmith if I'm us...</td>\n",
       "      <td>To set up tracing to LangSmith using LangChain...</td>\n",
       "      <td>None</td>\n",
       "      <td>To set up tracing to LangSmith while using Lan...</td>\n",
       "      <td>0</td>\n",
       "      <td>4.061921</td>\n",
       "      <td>9e85c6d6-adb7-4c3f-ae2c-9b5203b9cbc7</td>\n",
       "      <td>90940da2-3760-4fe0-b1a2-b81859a583b2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is LangSmith used for?</td>\n",
       "      <td>LangSmith is used for building production-grad...</td>\n",
       "      <td>None</td>\n",
       "      <td>LangSmith is a platform designed for the devel...</td>\n",
       "      <td>0</td>\n",
       "      <td>2.774645</td>\n",
       "      <td>b6cb2dbe-8d8b-4458-bf94-c37bbabe1d97</td>\n",
       "      <td>ddabc1b6-10c3-43ad-bd3f-8ce612f3928a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<ExperimentResults initial dataset version-b6c17146>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(\n",
    "    target_function,\n",
    "    data=dataset_name,  # Simplest and most reliable\n",
    "    evaluators=[is_concise_enough],\n",
    "    experiment_prefix=\"initial dataset version\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Found existing dataset: RAG Application Golden Dataset\n",
      " Current examples in dataset: 2\n",
      " Dataset already has examples\n",
      " Total examples in dataset: 2\n",
      "\n",
      " Starting evaluation...\n",
      "View the evaluation results for experiment: 'initial dataset version-d3e75f81' at:\n",
      "https://smith.langchain.com/o/679b2151-9692-435f-9a40-bc673cc68d61/datasets/d97e38c5-b6d8-4686-95c7-dcd88d379157/compare?selectedSessions=7de41579-ff82-4c8d-83b2-8c6c3dad5633\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00,  3.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Evaluation complete!\n",
      " Results: <ExperimentResults initial dataset version-d3e75f81>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langsmith import Client, evaluate\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"RAG Application Golden Dataset\"\n",
    "\n",
    "###examples are added \n",
    "\n",
    "\n",
    "try:\n",
    "    dataset = client.read_dataset(dataset_name=dataset_name)\n",
    "    print(f\" Found existing dataset: {dataset_name}\")\n",
    "except:\n",
    "    dataset = client.create_dataset(dataset_name=dataset_name)\n",
    "    print(f\" Created new dataset: {dataset_name}\")\n",
    "\n",
    "\n",
    "examples_to_add = [\n",
    "    {\n",
    "        \"question\": \"What is machine learning?\",\n",
    "        \"output\": \"Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is deep learning?\",\n",
    "        \"output\": \"Deep learning is a subset of machine learning that uses neural networks with multiple layers to analyze data.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is natural language processing?\",\n",
    "        \"output\": \"Natural language processing (NLP) is a field of AI that focuses on the interaction between computers and human language.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is a neural network?\",\n",
    "        \"output\": \"A neural network is a computing system inspired by biological neural networks that processes information through interconnected nodes.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is supervised learning?\",\n",
    "        \"output\": \"Supervised learning is a machine learning approach where models are trained on labeled data to make predictions.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Check if examples already exist\n",
    "existing_examples = list(client.list_examples(dataset_name=dataset_name))\n",
    "print(f\" Current examples in dataset: {len(existing_examples)}\")\n",
    "\n",
    "if len(existing_examples) == 0:\n",
    "    print(\" Adding examples to dataset...\")\n",
    "    for example in examples_to_add:\n",
    "        client.create_example(\n",
    "            inputs={\"question\": example[\"question\"]},\n",
    "            outputs={\"output\": example[\"output\"]},\n",
    "            dataset_id=dataset.id\n",
    "        )\n",
    "    print(f\" Added {len(examples_to_add)} examples to dataset\")\n",
    "else:\n",
    "    print(\" Dataset already has examples\")\n",
    "\n",
    "# Step 3: Verify dataset has examples\n",
    "examples = list(client.list_examples(dataset_name=dataset_name))\n",
    "print(f\" Total examples in dataset: {len(examples)}\")\n",
    "\n",
    "if len(examples) == 0:\n",
    "    print(\" Dataset has no examples! Cannot run evaluation.\")\n",
    "else:\n",
    "    # Step 4: Define evaluator\n",
    "    def is_concise_enough(run, example) -> dict:\n",
    "        \"\"\"Evaluator that checks if output is concise enough\"\"\"\n",
    "        prediction = run.outputs.get(\"output\", \"\")\n",
    "        reference = example.outputs.get(\"output\", \"\")\n",
    "        \n",
    "        if not reference:\n",
    "            return {\"key\": \"is_concise\", \"score\": 0}\n",
    "        \n",
    "        score = len(prediction) < 1.5 * len(reference)\n",
    "        return {\"key\": \"is_concise\", \"score\": int(score)}\n",
    "    \n",
    "    # Step 5: Define target function\n",
    "    def target_function(inputs: dict):\n",
    "        \"\"\"Your RAG function - replace with actual implementation\"\"\"\n",
    "        question = inputs[\"question\"]\n",
    "        \n",
    "        # TODO: Replace this with your actual RAG implementation\n",
    "        # For now, using a simple response\n",
    "        response = f\"This is a response to: {question}\"\n",
    "        \n",
    "        # Uncomment below when you have langsmith_rag function\n",
    "        # result = langsmith_rag(question)\n",
    "        # if isinstance(result, str):\n",
    "        #     return {\"output\": result}\n",
    "        # return result\n",
    "        \n",
    "        return {\"output\": response}\n",
    "    \n",
    "    # Step 6: Run evaluation\n",
    "    print(\"\\n Starting evaluation...\")\n",
    "    results = evaluate(\n",
    "        target_function,\n",
    "        data=dataset_name,\n",
    "        evaluators=[is_concise_enough],\n",
    "        experiment_prefix=\"initial dataset version\"\n",
    "    )\n",
    "    print(\" Evaluation complete!\")\n",
    "    print(f\" Results: {results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'initial dataset version-9e35c3dc' at:\n",
      "https://smith.langchain.com/o/679b2151-9692-435f-9a40-bc673cc68d61/datasets/d97e38c5-b6d8-4686-95c7-dcd88d379157/compare?selectedSessions=40530d2c-637b-4442-8e94-f2ae0234fabd\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:06,  3.35s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.question</th>\n",
       "      <th>outputs.output</th>\n",
       "      <th>error</th>\n",
       "      <th>reference.output</th>\n",
       "      <th>feedback.is_concise</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>example_id</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How do I set up tracing to LangSmith if I'm us...</td>\n",
       "      <td>To set up tracing to LangSmith using LangChain...</td>\n",
       "      <td>None</td>\n",
       "      <td>To set up tracing to LangSmith while using Lan...</td>\n",
       "      <td>0</td>\n",
       "      <td>3.633917</td>\n",
       "      <td>9e85c6d6-adb7-4c3f-ae2c-9b5203b9cbc7</td>\n",
       "      <td>2bd939bd-1f62-491a-a7ed-2a50a4231e3d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is LangSmith used for?</td>\n",
       "      <td>LangSmith is used for building production-grad...</td>\n",
       "      <td>None</td>\n",
       "      <td>LangSmith is a platform designed for the devel...</td>\n",
       "      <td>0</td>\n",
       "      <td>2.555561</td>\n",
       "      <td>b6cb2dbe-8d8b-4458-bf94-c37bbabe1d97</td>\n",
       "      <td>201653de-98fb-412b-8f15-9f8f348281dc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<ExperimentResults initial dataset version-9e35c3dc>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(\n",
    "    target_function,\n",
    "    data=dataset_name,  # <-- CHANGED THIS LINE\n",
    "    evaluators=[is_concise_enough],\n",
    "    experiment_prefix=\"initial dataset version\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataset Split\n",
    "\n",
    "You can run an experiment on a specific split of your dataset, let's try running on the Crucial Examples split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'Crucial Examples split-07aada19' at:\n",
      "https://smith.langchain.com/o/679b2151-9692-435f-9a40-bc673cc68d61/datasets/d97e38c5-b6d8-4686-95c7-dcd88d379157/compare?selectedSessions=b342e997-1a19-476c-8545-8ed92643400b\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:05,  2.96s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.question</th>\n",
       "      <th>outputs.output</th>\n",
       "      <th>error</th>\n",
       "      <th>reference.output</th>\n",
       "      <th>feedback.is_concise</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>example_id</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How do I set up tracing to LangSmith if I'm us...</td>\n",
       "      <td>To set up tracing to LangSmith when using Lang...</td>\n",
       "      <td>None</td>\n",
       "      <td>To set up tracing to LangSmith while using Lan...</td>\n",
       "      <td>0</td>\n",
       "      <td>3.283367</td>\n",
       "      <td>9e85c6d6-adb7-4c3f-ae2c-9b5203b9cbc7</td>\n",
       "      <td>8aca2243-5cfc-4a80-9f14-89953b162239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is LangSmith used for?</td>\n",
       "      <td>LangSmith is used for building production-grad...</td>\n",
       "      <td>None</td>\n",
       "      <td>LangSmith is a platform designed for the devel...</td>\n",
       "      <td>0</td>\n",
       "      <td>2.145844</td>\n",
       "      <td>b6cb2dbe-8d8b-4458-bf94-c37bbabe1d97</td>\n",
       "      <td>966b9e5b-80d6-443a-9cdf-32890c76e99f</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<ExperimentResults Crucial Examples split-07aada19>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(\n",
    "    target_function,\n",
    "    data=dataset_name,  # Simplest approach\n",
    "    evaluators=[is_concise_enough],\n",
    "    experiment_prefix=\"Crucial Examples split\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total examples: 2\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Client.update_example() got an unexpected keyword argument 'splits'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[78]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Step 3: Update examples with the split\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m example_id \u001b[38;5;129;01min\u001b[39;00m crucial_example_ids:\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate_example\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexample_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexample_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplits\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mCrucial Examples\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Add to the split\u001b[39;49;00m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m Added \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(crucial_example_ids)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m examples to \u001b[39m\u001b[33m'\u001b[39m\u001b[33mCrucial Examples\u001b[39m\u001b[33m'\u001b[39m\u001b[33m split\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Step 4: Now evaluate with the split\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Convert generator to list for the data parameter\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: Client.update_example() got an unexpected keyword argument 'splits'"
     ]
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"RAG Application Golden Dataset\"\n",
    "\n",
    "# Step 1: Get all examples\n",
    "examples = list(client.list_examples(dataset_name=dataset_name))\n",
    "print(f\"Total examples: {len(examples)}\")\n",
    "\n",
    "if len(examples) == 0:\n",
    "    print(\"⚠️ No examples in dataset!\")\n",
    "else:\n",
    "    # Step 2: Mark some examples as \"Crucial Examples\"\n",
    "    # Let's mark the first 3 examples as crucial\n",
    "    crucial_example_ids = [ex.id for ex in examples[:3]]\n",
    "    \n",
    "    # Step 3: Update examples with the split\n",
    "    for example_id in crucial_example_ids:\n",
    "        client.update_example(\n",
    "            example_id=example_id,\n",
    "            splits=[\"Crucial Examples\"]  # Add to the split\n",
    "        )\n",
    "    \n",
    "    print(f\" Added {len(crucial_example_ids)} examples to 'Crucial Examples' split\")\n",
    "    \n",
    "    # Step 4: Now evaluate with the split\n",
    "    # Convert generator to list for the data parameter\n",
    "    crucial_examples = list(\n",
    "        client.list_examples(\n",
    "            dataset_name=dataset_name, \n",
    "            splits=[\"Crucial Examples\"]\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    evaluate(\n",
    "        langsmith_rag,  # Your target function\n",
    "        data=crucial_examples,  # Pass as a list\n",
    "        evaluators=[is_concise_enough],  # Your evaluator\n",
    "        experiment_prefix=\"Crucial Examples split\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Update each example individually\n",
    "for example in examples[:2]:\n",
    "    # Get current metadata\n",
    "    current_metadata = example.metadata or {}\n",
    "    \n",
    "    # Update with split info in metadata\n",
    "    client.update_example(\n",
    "        example_id=example.id,\n",
    "        metadata={\n",
    "            **current_metadata,\n",
    "            \"split\": \"Crucial Examples\"\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples and their splits:\n",
      "  Example ID: b6cb2dbe-8d8b-4458-bf94-c37bbabe1d97, Splits: []\n",
      "  Example ID: 9e85c6d6-adb7-4c3f-ae2c-9b5203b9cbc7, Splits: []\n"
     ]
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"RAG Application Golden Dataset\"\n",
    "\n",
    "# List all examples and their splits\n",
    "examples = list(client.list_examples(dataset_name=dataset_name))\n",
    "\n",
    "print(\"Examples and their splits:\")\n",
    "for ex in examples:\n",
    "    splits = ex.splits if hasattr(ex, 'splits') else []\n",
    "    print(f\"  Example ID: {ex.id}, Splits: {splits}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 examples\n",
      "View the evaluation results for experiment: 'all examples-2b8dd322' at:\n",
      "https://smith.langchain.com/o/679b2151-9692-435f-9a40-bc673cc68d61/datasets/d97e38c5-b6d8-4686-95c7-dcd88d379157/compare?selectedSessions=25e45a1e-7cbd-45c4-9abd-9c2409e9426a\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:03,  3.02s/it]Error running target function: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************cFEA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1923, in _forward\n",
      "    fn(*args, langsmith_extra=langsmith_extra)\n",
      "  File \"/var/folders/0k/l8w_v7614tb84g7thk3tbbhm0000gn/T/ipykernel_10571/117198349.py\", line 19, in target_function\n",
      "    result = langsmith_rag(inputs[\"question\"])\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/0k/l8w_v7614tb84g7thk3tbbhm0000gn/T/ipykernel_10571/252468420.py\", line 110, in langsmith_rag\n",
      "    documents = retrieve_documents(question)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/0k/l8w_v7614tb84g7thk3tbbhm0000gn/T/ipykernel_10571/252468420.py\", line 64, in retrieve_documents\n",
      "    return retriever.invoke(question)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/langchain_core/retrievers.py\", line 263, in invoke\n",
      "    result = self._get_relevant_documents(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/langchain_core/vectorstores/base.py\", line 1067, in _get_relevant_documents\n",
      "    docs = self.vectorstore.similarity_search(query, **kwargs_)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/langchain_community/vectorstores/sklearn.py\", line 256, in similarity_search\n",
      "    docs_scores = self.similarity_search_with_score(query, k=k, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/langchain_community/vectorstores/sklearn.py\", line 238, in similarity_search_with_score\n",
      "    query_embedding = self._embedding_function.embed_query(query)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/langchain_openai/embeddings/base.py\", line 639, in embed_query\n",
      "    return self.embed_documents([text], **kwargs)[0]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/langchain_openai/embeddings/base.py\", line 591, in embed_documents\n",
      "    return self._get_len_safe_embeddings(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/langchain_openai/embeddings/base.py\", line 479, in _get_len_safe_embeddings\n",
      "    response = self.client.create(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/openai/resources/embeddings.py\", line 132, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.AuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************cFEA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n",
      "Error running evaluator <DynamicRunEvaluator is_concise_enough> on run 628ef600-2915-491d-b950-13daca7547bc: TypeError(\"object of type 'NoneType' has no len()\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1619, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/0k/l8w_v7614tb84g7thk3tbbhm0000gn/T/ipykernel_10571/117198349.py\", line 15, in is_concise_enough\n",
      "    score = len(outputs[\"output\"]) < 1.5 * len(reference_outputs[\"output\"])\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "2it [00:03,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Evaluation complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langsmith import Client, evaluate\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"RAG Application Golden Dataset\"\n",
    "\n",
    "# Check if any examples exist\n",
    "examples = list(client.list_examples(dataset_name=dataset_name))\n",
    "print(f\"Found {len(examples)} examples\")\n",
    "\n",
    "if len(examples) == 0:\n",
    "    print(\" No examples in dataset! Add examples first.\")\n",
    "else:\n",
    "    # Define evaluator and target function\n",
    "    def is_concise_enough(reference_outputs: dict, outputs: dict) -> dict:\n",
    "        score = len(outputs[\"output\"]) < 1.5 * len(reference_outputs[\"output\"])\n",
    "        return {\"key\": \"is_concise\", \"score\": int(score)}\n",
    "    \n",
    "    def target_function(inputs: dict):\n",
    "        result = langsmith_rag(inputs[\"question\"])\n",
    "        if isinstance(result, str):\n",
    "            return {\"output\": result}\n",
    "        return result\n",
    "    \n",
    "    # Evaluate WITHOUT splits (will use all examples)\n",
    "    results = evaluate(\n",
    "        target_function,\n",
    "        data=dataset_name,  # Use all examples\n",
    "        evaluators=[is_concise_enough],\n",
    "        experiment_prefix=\"all examples\"\n",
    "    )\n",
    "    print(\" Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total examples: 2\n",
      " Marked 2 examples as crucial\n",
      "View the evaluation results for experiment: 'Crucial Examples-f9eff15b' at:\n",
      "https://smith.langchain.com/o/679b2151-9692-435f-9a40-bc673cc68d61/datasets/d97e38c5-b6d8-4686-95c7-dcd88d379157/compare?selectedSessions=d3553440-dc58-46f2-93f0-ab6ec3b8f1cb\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Error running target function: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************cFEA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1923, in _forward\n",
      "    fn(*args, langsmith_extra=langsmith_extra)\n",
      "  File \"/var/folders/0k/l8w_v7614tb84g7thk3tbbhm0000gn/T/ipykernel_10571/117198349.py\", line 19, in target_function\n",
      "    result = langsmith_rag(inputs[\"question\"])\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/0k/l8w_v7614tb84g7thk3tbbhm0000gn/T/ipykernel_10571/252468420.py\", line 110, in langsmith_rag\n",
      "    documents = retrieve_documents(question)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/0k/l8w_v7614tb84g7thk3tbbhm0000gn/T/ipykernel_10571/252468420.py\", line 64, in retrieve_documents\n",
      "    return retriever.invoke(question)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/langchain_core/retrievers.py\", line 263, in invoke\n",
      "    result = self._get_relevant_documents(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/langchain_core/vectorstores/base.py\", line 1067, in _get_relevant_documents\n",
      "    docs = self.vectorstore.similarity_search(query, **kwargs_)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/langchain_community/vectorstores/sklearn.py\", line 256, in similarity_search\n",
      "    docs_scores = self.similarity_search_with_score(query, k=k, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/langchain_community/vectorstores/sklearn.py\", line 238, in similarity_search_with_score\n",
      "    query_embedding = self._embedding_function.embed_query(query)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/langchain_openai/embeddings/base.py\", line 639, in embed_query\n",
      "    return self.embed_documents([text], **kwargs)[0]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/langchain_openai/embeddings/base.py\", line 591, in embed_documents\n",
      "    return self._get_len_safe_embeddings(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/langchain_openai/embeddings/base.py\", line 479, in _get_len_safe_embeddings\n",
      "    response = self.client.create(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/openai/resources/embeddings.py\", line 132, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.AuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************cFEA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n",
      "Error running evaluator <DynamicRunEvaluator is_concise_enough> on run 92f11a65-b837-4e52-8215-53ab9aa263ec: TypeError(\"object of type 'NoneType' has no len()\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1619, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/0k/l8w_v7614tb84g7thk3tbbhm0000gn/T/ipykernel_10571/117198349.py\", line 15, in is_concise_enough\n",
      "    score = len(outputs[\"output\"]) < 1.5 * len(reference_outputs[\"output\"])\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "1it [00:00,  2.53it/s]Error running target function: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************cFEA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1923, in _forward\n",
      "    fn(*args, langsmith_extra=langsmith_extra)\n",
      "  File \"/var/folders/0k/l8w_v7614tb84g7thk3tbbhm0000gn/T/ipykernel_10571/117198349.py\", line 19, in target_function\n",
      "    result = langsmith_rag(inputs[\"question\"])\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/0k/l8w_v7614tb84g7thk3tbbhm0000gn/T/ipykernel_10571/252468420.py\", line 110, in langsmith_rag\n",
      "    documents = retrieve_documents(question)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/0k/l8w_v7614tb84g7thk3tbbhm0000gn/T/ipykernel_10571/252468420.py\", line 64, in retrieve_documents\n",
      "    return retriever.invoke(question)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/langchain_core/retrievers.py\", line 263, in invoke\n",
      "    result = self._get_relevant_documents(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/langchain_core/vectorstores/base.py\", line 1067, in _get_relevant_documents\n",
      "    docs = self.vectorstore.similarity_search(query, **kwargs_)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/langchain_community/vectorstores/sklearn.py\", line 256, in similarity_search\n",
      "    docs_scores = self.similarity_search_with_score(query, k=k, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/langchain_community/vectorstores/sklearn.py\", line 238, in similarity_search_with_score\n",
      "    query_embedding = self._embedding_function.embed_query(query)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/langchain_openai/embeddings/base.py\", line 639, in embed_query\n",
      "    return self.embed_documents([text], **kwargs)[0]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/langchain_openai/embeddings/base.py\", line 591, in embed_documents\n",
      "    return self._get_len_safe_embeddings(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/langchain_openai/embeddings/base.py\", line 479, in _get_len_safe_embeddings\n",
      "    response = self.client.create(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/openai/resources/embeddings.py\", line 132, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.AuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************cFEA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n",
      "Error running evaluator <DynamicRunEvaluator is_concise_enough> on run 044096b6-cf3f-4c63-a89b-2afc2d9949c8: TypeError(\"object of type 'NoneType' has no len()\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1619, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/0k/l8w_v7614tb84g7thk3tbbhm0000gn/T/ipykernel_10571/117198349.py\", line 15, in is_concise_enough\n",
      "    score = len(outputs[\"output\"]) < 1.5 * len(reference_outputs[\"output\"])\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "2it [00:01,  1.55it/s]\n"
     ]
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"RAG Application Golden Dataset\"\n",
    "\n",
    "examples = list(client.list_examples(dataset_name=dataset_name))\n",
    "print(f\"Total examples: {len(examples)}\")\n",
    "\n",
    "if len(examples) > 0:\n",
    "    crucial_example_ids = [ex.id for ex in examples[:3]]\n",
    "    \n",
    "    # Use metadata to mark crucial examples\n",
    "    for example_id in crucial_example_ids:\n",
    "        client.update_example(\n",
    "            example_id=example_id,\n",
    "            metadata={\"is_crucial\": True}\n",
    "        )\n",
    "    \n",
    "    print(f\" Marked {len(crucial_example_ids)} examples as crucial\")\n",
    "    \n",
    "    # Evaluate using metadata filter\n",
    "    evaluate(\n",
    "        target_function,\n",
    "        data=client.list_examples(\n",
    "            dataset_name=dataset_name,\n",
    "            metadata={\"is_crucial\": True}\n",
    "        ),\n",
    "        evaluators=[is_concise_enough],\n",
    "        experiment_prefix=\"Crucial Examples\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mStopIteration\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[70]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlist_examples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplits\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mCrucial Examples\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# We pass in a list of Splits\u001b[39;49;00m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevaluators\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mis_concise_enough\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexperiment_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mCrucial Examples split\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/langsmith/evaluation/_runner.py:423\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(target, data, evaluators, summary_evaluators, metadata, experiment_prefix, description, max_concurrency, num_repetitions, client, blocking, experiment, upload_results, error_handling, **kwargs)\u001b[39m\n\u001b[32m    421\u001b[39m     _warn_once(\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mupload_results\u001b[39m\u001b[33m'\u001b[39m\u001b[33m parameter is in beta.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    422\u001b[39m logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRunning evaluation over target system \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m423\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    424\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    425\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    426\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevaluators\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mOptional\u001b[49m\u001b[43m[\u001b[49m\u001b[43mSequence\u001b[49m\u001b[43m[\u001b[49m\u001b[43mEVALUATOR_T\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluators\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[43m    \u001b[49m\u001b[43msummary_evaluators\u001b[49m\u001b[43m=\u001b[49m\u001b[43msummary_evaluators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexperiment_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexperiment_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    430\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_concurrency\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_concurrency\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_repetitions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_repetitions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    433\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[43m    \u001b[49m\u001b[43mblocking\u001b[49m\u001b[43m=\u001b[49m\u001b[43mblocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexperiment\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexperiment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m    \u001b[49m\u001b[43mupload_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mupload_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m    \u001b[49m\u001b[43merror_handling\u001b[49m\u001b[43m=\u001b[49m\u001b[43merror_handling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/langsmith/evaluation/_runner.py:1081\u001b[39m, in \u001b[36m_evaluate\u001b[39m\u001b[34m(target, data, evaluators, summary_evaluators, metadata, experiment_prefix, description, max_concurrency, num_repetitions, client, blocking, experiment, upload_results, error_handling)\u001b[39m\n\u001b[32m   1065\u001b[39m runs = \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m _is_callable(target) \u001b[38;5;28;01melse\u001b[39;00m cast(Iterable[schemas.Run], target)\n\u001b[32m   1066\u001b[39m experiment_, runs = _resolve_experiment(experiment, runs, client)\n\u001b[32m   1068\u001b[39m manager = \u001b[43m_ExperimentManager\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1069\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1070\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1071\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1072\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexperiment\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexperiment_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexperiment_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1073\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1074\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_repetitions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_repetitions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1075\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If provided, we don't need to create a new experiment.\u001b[39;49;00m\n\u001b[32m   1076\u001b[39m \u001b[43m    \u001b[49m\u001b[43mruns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mruns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1077\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Create or resolve the experiment.\u001b[39;49;00m\n\u001b[32m   1078\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude_attachments\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_include_attachments\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluators\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1079\u001b[39m \u001b[43m    \u001b[49m\u001b[43mupload_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mupload_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1080\u001b[39m \u001b[43m    \u001b[49m\u001b[43merror_handling\u001b[49m\u001b[43m=\u001b[49m\u001b[43merror_handling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m1081\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1082\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cache_dir := ls_utils.get_cache_dir(\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   1083\u001b[39m     cache_path = pathlib.Path(cache_dir) / \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmanager.dataset_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.yaml\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/langsmith/evaluation/_runner.py:1454\u001b[39m, in \u001b[36m_ExperimentManager.start\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1453\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstart\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> _ExperimentManager:\n\u001b[32m-> \u001b[39m\u001b[32m1454\u001b[39m     first_example = \u001b[38;5;28mnext\u001b[39m(itertools.islice(\u001b[38;5;28mself\u001b[39m.examples, \u001b[32m1\u001b[39m))\n\u001b[32m   1455\u001b[39m     project = \u001b[38;5;28mself\u001b[39m._get_project(first_example) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._upload_results \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1456\u001b[39m     \u001b[38;5;28mself\u001b[39m._print_experiment_start(project, first_example)\n",
      "\u001b[31mStopIteration\u001b[39m: "
     ]
    }
   ],
   "source": [
    "evaluate(\n",
    "    target_function,\n",
    "    data=client.list_examples(dataset_name=dataset_name, splits=[\"Crucial Examples\"]),  # We pass in a list of Splits\n",
    "    evaluators=[is_concise_enough],\n",
    "    experiment_prefix=\"Crucial Examples split\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Specific Data Points\n",
    "\n",
    "You can specify individual data points to run an experiment over as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching examples from dataset...\n",
      "\n",
      "Total examples in dataset: 2\n",
      "\n",
      "======================================================================\n",
      "AVAILABLE EXAMPLES\n",
      "======================================================================\n",
      "\n",
      "1. Example ID: 9e85c6d6-adb7-4c3f-ae2c-9b5203b9cbc7\n",
      "   Question: How do I set up tracing to LangSmith if I'm using LangChain?\n",
      "   Answer: To set up tracing to LangSmith while using LangChain, you need to set the environment variable `LANGSMITH_TRACING` to 'true'. Additionally, you must set the `LANGSMITH_API_KEY` environment variable to your API key.\n",
      "\n",
      "2. Example ID: b6cb2dbe-8d8b-4458-bf94-c37bbabe1d97\n",
      "   Question: What is LangSmith used for?\n",
      "   Answer: LangSmith is a platform designed for the development, monitoring, and testing of LLM applications.\n",
      "\n",
      "======================================================================\n",
      "SELECTED EXAMPLES FOR EVALUATION\n",
      "======================================================================\n",
      "Example 1: 9e85c6d6-adb7-4c3f-ae2c-9b5203b9cbc7\n",
      "Example 2: b6cb2dbe-8d8b-4458-bf94-c37bbabe1d97\n",
      "\n",
      "======================================================================\n",
      "RUNNING EVALUATION\n",
      "======================================================================\n",
      "View the evaluation results for experiment: 'two specific example ids-cd3d2c4c' at:\n",
      "https://smith.langchain.com/o/679b2151-9692-435f-9a40-bc673cc68d61/datasets/d97e38c5-b6d8-4686-95c7-dcd88d379157/compare?selectedSessions=741e98ac-e6d6-47e5-b993-80946f3828ce\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Error running target function: argument 'text': 'dict' object cannot be converted to 'PyString'\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1923, in _forward\n",
      "    fn(*args, langsmith_extra=langsmith_extra)\n",
      "  File \"/var/folders/0k/l8w_v7614tb84g7thk3tbbhm0000gn/T/ipykernel_10571/252468420.py\", line 110, in langsmith_rag\n",
      "    documents = retrieve_documents(question)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/0k/l8w_v7614tb84g7thk3tbbhm0000gn/T/ipykernel_10571/252468420.py\", line 64, in retrieve_documents\n",
      "    return retriever.invoke(question)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/langchain_core/retrievers.py\", line 263, in invoke\n",
      "    result = self._get_relevant_documents(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/langchain_core/vectorstores/base.py\", line 1067, in _get_relevant_documents\n",
      "    docs = self.vectorstore.similarity_search(query, **kwargs_)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/langchain_community/vectorstores/sklearn.py\", line 256, in similarity_search\n",
      "    docs_scores = self.similarity_search_with_score(query, k=k, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/langchain_community/vectorstores/sklearn.py\", line 238, in similarity_search_with_score\n",
      "    query_embedding = self._embedding_function.embed_query(query)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/langchain_openai/embeddings/base.py\", line 639, in embed_query\n",
      "    return self.embed_documents([text], **kwargs)[0]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/langchain_openai/embeddings/base.py\", line 591, in embed_documents\n",
      "    return self._get_len_safe_embeddings(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/langchain_openai/embeddings/base.py\", line 476, in _get_len_safe_embeddings\n",
      "    _iter, tokens, indices = self._tokenize(texts, _chunk_size)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/langchain_openai/embeddings/base.py\", line 431, in _tokenize\n",
      "    token = encoding.encode_ordinary(text)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/tiktoken/core.py\", line 73, in encode_ordinary\n",
      "    return self._core_bpe.encode_ordinary(text)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: argument 'text': 'dict' object cannot be converted to 'PyString'\n",
      "Error running evaluator <DynamicRunEvaluator is_concise_enough> on run 795af16e-1426-4e4e-b87a-d817bdeb6d96: TypeError(\"object of type 'NoneType' has no len()\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1619, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"/var/folders/0k/l8w_v7614tb84g7thk3tbbhm0000gn/T/ipykernel_10571/1323898595.py\", line 38, in is_concise_enough\n",
      "    is_concise = len(prediction) < 150\n",
      "                 ^^^^^^^^^^^^^^^\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "Error running target function: argument 'text': 'dict' object cannot be converted to 'PyString'\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1923, in _forward\n",
      "    fn(*args, langsmith_extra=langsmith_extra)\n",
      "  File \"/var/folders/0k/l8w_v7614tb84g7thk3tbbhm0000gn/T/ipykernel_10571/252468420.py\", line 110, in langsmith_rag\n",
      "    documents = retrieve_documents(question)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/0k/l8w_v7614tb84g7thk3tbbhm0000gn/T/ipykernel_10571/252468420.py\", line 64, in retrieve_documents\n",
      "    return retriever.invoke(question)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/langchain_core/retrievers.py\", line 263, in invoke\n",
      "    result = self._get_relevant_documents(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/langchain_core/vectorstores/base.py\", line 1067, in _get_relevant_documents\n",
      "    docs = self.vectorstore.similarity_search(query, **kwargs_)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/langchain_community/vectorstores/sklearn.py\", line 256, in similarity_search\n",
      "    docs_scores = self.similarity_search_with_score(query, k=k, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/langchain_community/vectorstores/sklearn.py\", line 238, in similarity_search_with_score\n",
      "    query_embedding = self._embedding_function.embed_query(query)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/langchain_openai/embeddings/base.py\", line 639, in embed_query\n",
      "    return self.embed_documents([text], **kwargs)[0]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/langchain_openai/embeddings/base.py\", line 591, in embed_documents\n",
      "    return self._get_len_safe_embeddings(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/langchain_openai/embeddings/base.py\", line 476, in _get_len_safe_embeddings\n",
      "    _iter, tokens, indices = self._tokenize(texts, _chunk_size)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/langchain_openai/embeddings/base.py\", line 431, in _tokenize\n",
      "    token = encoding.encode_ordinary(text)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/tiktoken/core.py\", line 73, in encode_ordinary\n",
      "    return self._core_bpe.encode_ordinary(text)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: argument 'text': 'dict' object cannot be converted to 'PyString'\n",
      "Error running evaluator <DynamicRunEvaluator is_concise_enough> on run 030cec68-7668-401b-b9ca-da0800894adb: TypeError(\"object of type 'NoneType' has no len()\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1619, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"/var/folders/0k/l8w_v7614tb84g7thk3tbbhm0000gn/T/ipykernel_10571/1323898595.py\", line 38, in is_concise_enough\n",
      "    is_concise = len(prediction) < 150\n",
      "                 ^^^^^^^^^^^^^^^\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "2it [00:00,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"RAG Application Golden Dataset\"\n",
    "\n",
    "# Step 1: List all examples and get their IDs\n",
    "print(\"Fetching examples from dataset...\")\n",
    "examples = list(client.list_examples(dataset_name=dataset_name))\n",
    "\n",
    "print(f\"\\nTotal examples in dataset: {len(examples)}\")\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"AVAILABLE EXAMPLES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Display all examples with their IDs\n",
    "for i, ex in enumerate(examples, 1):\n",
    "    print(f\"\\n{i}. Example ID: {ex.id}\")\n",
    "    print(f\"   Question: {ex.inputs.get('question', 'N/A')}\")\n",
    "    if hasattr(ex, 'outputs') and ex.outputs:\n",
    "        print(f\"   Answer: {ex.outputs.get('answer', ex.outputs.get('output', 'N/A'))}\")\n",
    "\n",
    "# Step 2: Select specific example IDs (first two)\n",
    "if len(examples) >= 2:\n",
    "    example_id_1 = examples[0].id\n",
    "    example_id_2 = examples[1].id\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"SELECTED EXAMPLES FOR EVALUATION\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Example 1: {example_id_1}\")\n",
    "    print(f\"Example 2: {example_id_2}\")\n",
    "    \n",
    "    # Step 3: Define evaluator\n",
    "    def is_concise_enough(run, example) -> dict:\n",
    "        \"\"\"Check if answer is concise (under 150 characters)\"\"\"\n",
    "        prediction = run.outputs.get(\"output\", run.outputs.get(\"answer\", \"\"))\n",
    "        is_concise = len(prediction) < 150\n",
    "        \n",
    "        return {\n",
    "            \"key\": \"is_concise\",\n",
    "            \"score\": 1 if is_concise else 0,\n",
    "            \"comment\": f\"Length: {len(prediction)} chars\"\n",
    "        }\n",
    "    \n",
    "    # Step 4: Run evaluation on specific examples\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"RUNNING EVALUATION\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    results = evaluate(\n",
    "        langsmith_rag,  # Your RAG function\n",
    "        data=client.list_examples(\n",
    "            dataset_name=dataset_name, \n",
    "            example_ids=[\n",
    "                example_id_1,\n",
    "                example_id_2\n",
    "            ]\n",
    "        ),\n",
    "        evaluators=[is_concise_enough],\n",
    "        experiment_prefix=\"two specific example ids\"\n",
    "    )\n",
    "    \n",
    "    print(\"\\nEvaluation completed!\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\nNeed at least 2 examples in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Repetitions\n",
    "\n",
    "You can run an experiment several times to make sure you have consistent results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'two repetitions-c5c98699' at:\n",
      "https://smith.langchain.com/o/679b2151-9692-435f-9a40-bc673cc68d61/datasets/d97e38c5-b6d8-4686-95c7-dcd88d379157/compare?selectedSessions=9f139f89-0ad8-44c7-afed-f6afcc76d599\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:11,  2.86s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.question</th>\n",
       "      <th>outputs.output</th>\n",
       "      <th>error</th>\n",
       "      <th>reference.output</th>\n",
       "      <th>feedback.is_concise</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>example_id</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How do I set up tracing to LangSmith if I'm us...</td>\n",
       "      <td>To set up tracing to LangSmith using LangChain...</td>\n",
       "      <td>None</td>\n",
       "      <td>To set up tracing to LangSmith while using Lan...</td>\n",
       "      <td>0</td>\n",
       "      <td>3.031042</td>\n",
       "      <td>9e85c6d6-adb7-4c3f-ae2c-9b5203b9cbc7</td>\n",
       "      <td>1e6a1667-ddd3-47c5-b897-29fdaf7fc958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is LangSmith used for?</td>\n",
       "      <td>LangSmith is used for building production-grad...</td>\n",
       "      <td>None</td>\n",
       "      <td>LangSmith is a platform designed for the devel...</td>\n",
       "      <td>0</td>\n",
       "      <td>2.230294</td>\n",
       "      <td>b6cb2dbe-8d8b-4458-bf94-c37bbabe1d97</td>\n",
       "      <td>208c53d1-e21c-4ecb-bbfd-26689a2f8404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How do I set up tracing to LangSmith if I'm us...</td>\n",
       "      <td>To set up tracing to LangSmith with LangChain,...</td>\n",
       "      <td>None</td>\n",
       "      <td>To set up tracing to LangSmith while using Lan...</td>\n",
       "      <td>0</td>\n",
       "      <td>3.167418</td>\n",
       "      <td>9e85c6d6-adb7-4c3f-ae2c-9b5203b9cbc7</td>\n",
       "      <td>c4cac408-61c2-4a04-8f8d-2826213ffbd2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is LangSmith used for?</td>\n",
       "      <td>LangSmith is used for building production-grad...</td>\n",
       "      <td>None</td>\n",
       "      <td>LangSmith is a platform designed for the devel...</td>\n",
       "      <td>0</td>\n",
       "      <td>2.311113</td>\n",
       "      <td>b6cb2dbe-8d8b-4458-bf94-c37bbabe1d97</td>\n",
       "      <td>56faeafb-cc4b-43ae-93ac-8e80707047ca</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<ExperimentResults two repetitions-c5c98699>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(\n",
    "    target_function,\n",
    "    data=dataset_name,\n",
    "    evaluators=[is_concise_enough],\n",
    "    experiment_prefix=\"two repetitions\",\n",
    "    num_repetitions=2   # This field defaults to 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Concurrency\n",
    "You can also kick off concurrent threads of execution to make your experiments finish faster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'concurrency-caed46f8' at:\n",
      "https://smith.langchain.com/o/679b2151-9692-435f-9a40-bc673cc68d61/datasets/d97e38c5-b6d8-4686-95c7-dcd88d379157/compare?selectedSessions=f31ee50c-a62e-4b8e-bde1-a6ccf969bd51\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00,  3.56it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.question</th>\n",
       "      <th>outputs.output</th>\n",
       "      <th>error</th>\n",
       "      <th>reference.output</th>\n",
       "      <th>feedback.is_concise</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>example_id</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is LangSmith used for?</td>\n",
       "      <td>This is a response to: What is LangSmith used ...</td>\n",
       "      <td>None</td>\n",
       "      <td>LangSmith is a platform designed for the devel...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002934</td>\n",
       "      <td>b6cb2dbe-8d8b-4458-bf94-c37bbabe1d97</td>\n",
       "      <td>c25854f2-a8f1-4938-bcf5-c297d24bf3c7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How do I set up tracing to LangSmith if I'm us...</td>\n",
       "      <td>This is a response to: How do I set up tracing...</td>\n",
       "      <td>None</td>\n",
       "      <td>To set up tracing to LangSmith while using Lan...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000893</td>\n",
       "      <td>9e85c6d6-adb7-4c3f-ae2c-9b5203b9cbc7</td>\n",
       "      <td>4b265411-0bf9-42d0-8929-5b440af19ef7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<ExperimentResults concurrency-caed46f8>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(\n",
    "    target_function,\n",
    "    data=dataset_name,\n",
    "    evaluators=[is_concise_enough],\n",
    "    experiment_prefix=\"concurrency\",\n",
    "    max_concurrency=3,  # This defaults to None, so this is an improvement!\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Metadata \n",
    "\n",
    "You can (and should) add metadata to your experiments, to make them easier to find in the UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'metadata added-3d119826' at:\n",
      "https://smith.langchain.com/o/679b2151-9692-435f-9a40-bc673cc68d61/datasets/d97e38c5-b6d8-4686-95c7-dcd88d379157/compare?selectedSessions=50902942-1696-4a36-ad32-b8703a43487d\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00,  3.24it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.question</th>\n",
       "      <th>outputs.output</th>\n",
       "      <th>error</th>\n",
       "      <th>reference.output</th>\n",
       "      <th>feedback.is_concise</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>example_id</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is LangSmith used for?</td>\n",
       "      <td>This is a response to: What is LangSmith used ...</td>\n",
       "      <td>None</td>\n",
       "      <td>LangSmith is a platform designed for the devel...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000826</td>\n",
       "      <td>b6cb2dbe-8d8b-4458-bf94-c37bbabe1d97</td>\n",
       "      <td>1fd4bc12-33f2-4912-b4c8-b23fa3221ae6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How do I set up tracing to LangSmith if I'm us...</td>\n",
       "      <td>This is a response to: How do I set up tracing...</td>\n",
       "      <td>None</td>\n",
       "      <td>To set up tracing to LangSmith while using Lan...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>9e85c6d6-adb7-4c3f-ae2c-9b5203b9cbc7</td>\n",
       "      <td>eff8a6e9-dac6-498e-a65c-2867df57dc44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<ExperimentResults metadata added-3d119826>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(\n",
    "    target_function,\n",
    "    data=dataset_name,\n",
    "    evaluators=[is_concise_enough],\n",
    "    experiment_prefix=\"metadata added\",\n",
    "    metadata={  # We can pass custom metadata for the experiment, such as the model name\n",
    "        \"model_name\": MODEL_NAME\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
